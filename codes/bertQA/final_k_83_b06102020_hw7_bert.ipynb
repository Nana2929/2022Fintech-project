{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "final_k.83_b06102020_hw7_bert.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TJ1fSAJE2oaC",
        "TevOvhC03m0h"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvSGDbExff_I"
      },
      "source": [
        "# **Homework 7 - Bert (Question Answering)**\n",
        "\n",
        "If you have any questions, feel free to email us at ntu-ml-2021spring-ta@googlegroups.com\n",
        "\n",
        "\n",
        "\n",
        "Slide:    [Link](https://docs.google.com/presentation/d/1aQoWogAQo_xVJvMQMrGaYiWzuyfO0QyLLAhiMwFyS2w)　Kaggle: [Link](https://www.kaggle.com/c/ml2021-spring-hw7)　Data: [Link](https://drive.google.com/uc?id=1znKmX08v9Fygp-dgwo7BKiLIf2qL1FH1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGOr_eS3wJJf"
      },
      "source": [
        "## Task description\n",
        "- Chinese Extractive Question Answering\n",
        "  - Input: Paragraph + Question\n",
        "  - Output: Answer\n",
        "\n",
        "- Objective: Learn how to fine tune a pretrained model on downstream task using transformers\n",
        "\n",
        "- Todo\n",
        "    - Fine tune a pretrained chinese BERT model\n",
        "    - Change hyperparameters (e.g. doc_stride)\n",
        "    - Apply linear learning rate decay\n",
        "    - Try other pretrained models\n",
        "    - Improve preprocessing\n",
        "    - Improve postprocessing\n",
        "- Training tips\n",
        "    - Automatic mixed precision\n",
        "    - Gradient accumulation\n",
        "    - Ensemble\n",
        "\n",
        "- Estimated training time (tesla t4 with automatic mixed precision enabled)\n",
        "    - Simple: 8mins\n",
        "    - Medium: 8mins\n",
        "    - Strong: 25mins\n",
        "    - Boss: 2hrs\n",
        "# References: \n",
        "1. https://github.com/PyTorchLightning/lightning-bolts\n",
        "2. https://huggingface.co/models\n",
        "3. https://github.com/ildoonet/pytorch-gradual-warmup-lr\n",
        "4. grad_accum:\n",
        "https://kozodoi.me/python/deep%20learning/pytorch/tutorial/2021/02/19/gradient-accumulation.html \n",
        "5. Regex matching (post-processing: match [UNK] back)\n",
        "    https://regexr.com/3f4vo\n",
        "\n",
        "    https://docs.python.org/3/library/re.html\n",
        "\n",
        "    https://www.programiz.com/python-programming/regex\n",
        "\n",
        "    https://transbiz.com.tw/\n",
        "    regex-regular-expression-ga-%E6%AD%A3%E8%A6%8F%E8%A1%A8%E7%A4%BA%E5%BC%8F/\n",
        "\n",
        "    https://blog.finxter.com/python-regex-greedy-vs-non-greedy-quantifiers/\n",
        "6. Models \n",
        "(Distilbert does not support fasttokenizer, token_type_ids)\n",
        "https://github.com/huggingface/transformers/issues/2702\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ1fSAJE2oaC"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPrc4Eie9Yo5",
        "outputId": "7fa15cc3-a0ce-4a40-98b0-35dbf7a58ace"
      },
      "source": [
        "# Download link 1\n",
        "!gdown --id '1znKmX08v9Fygp-dgwo7BKiLIf2qL1FH1' --output hw7_data.zip\n",
        "\n",
        "# Download Link 2 (if the above link fails) \n",
        "# !gdown --id '1pOu3FdPdvzielUZyggeD7KDnVy9iW1uC' --output hw7_data.zip\n",
        "\n",
        "!unzip -o hw7_data.zip\n",
        "\n",
        "# For this HW, K80 < P4 < T4 < P100 <= T4(fp16) < V100"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1znKmX08v9Fygp-dgwo7BKiLIf2qL1FH1\n",
            "To: /content/hw7_data.zip\n",
            "\r0.00B [00:00, ?B/s]\r7.71MB [00:00, 67.8MB/s]\n",
            "Archive:  hw7_data.zip\n",
            "  inflating: hw7_dev.json            \n",
            "  inflating: hw7_test.json           \n",
            "  inflating: hw7_train.json          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TevOvhC03m0h"
      },
      "source": [
        "## Install transformers\n",
        "\n",
        "Documentation for the toolkit:　https://huggingface.co/transformers/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbxWFX_jpDom",
        "outputId": "9ab9b2c9-d293-4d39-ed26-08b5d489cec0"
      },
      "source": [
        "# You are allowed to change version of transformers or use other toolkits\n",
        "!pip install transformers==4.5.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |███████████████████▍            | 542kB 26.0MB/s eta 0:00:01"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dKM4yCh4LI_"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOTHHtWJoahe"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset \n",
        "from transformers import AdamW, BertForQuestionAnswering, BertTokenizerFast\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Fix random seed for reproducibility\n",
        "def same_seeds(seed):\n",
        "\t  torch.manual_seed(seed)\n",
        "\t  if torch.cuda.is_available():\n",
        "\t\t    torch.cuda.manual_seed(seed)\n",
        "\t\t    torch.cuda.manual_seed_all(seed)\n",
        "\t  np.random.seed(seed)\n",
        "\t  random.seed(seed)\n",
        "\t  torch.backends.cudnn.benchmark = False\n",
        "\t  torch.backends.cudnn.deterministic = True\n",
        "same_seeds(2929)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pBtSZP1SKQO"
      },
      "source": [
        "# Change \"fp16_training\" to True to support automatic mixed precision training (fp16)\t\n",
        "fp16_training = True\n",
        "\n",
        "if fp16_training:\n",
        "    !pip install accelerate==0.2.0\n",
        "    from accelerate import Accelerator\n",
        "    accelerator = Accelerator(fp16=True)\n",
        "    device = accelerator.device\n",
        "\n",
        "# Documentation for the toolkit:  https://huggingface.co/docs/accelerate/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3eR2yJWWHqz",
        "outputId": "bb951921-6433-40e0-f2e0-30e1c883c597"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat May 22 12:33:07 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YgXHuVLp_6j"
      },
      "source": [
        "## Load Model and Tokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyBCYGjAp3ym"
      },
      "source": [
        "# You can safely ignore the warning message (it pops up because new prediction heads for QA are initialized randomly)\n",
        "from transformers import AutoModelForQuestionAnswering, BertTokenizer, AutoModel, AlbertForMaskedLM\n",
        "# bert-base-chinese  sample code \n",
        "# wptoux/albert-chinese-large-qa good\n",
        "# hfl/chinese-roberta-wwm-ext good \n",
        "# hfl/chinese-roberta-wwm-ext-large good!\n",
        "# hfl/chinese-electra-180g-small-discriminator nah\n",
        "# yechen/question-answering-chinese nah\n",
        "\n",
        "# ckiplab/albert-base-chinese forward got unexpected start position\n",
        "# ckiplab/albert-base-chinese-pos: forward() got an unexpected keyword argument 'start_positions'\n",
        "\n",
        "# model = AutoModelForQuestionAnswering.from_pretrained('wptoux/albert-chinese-large-qa')\n",
        "# sample code only covers preproc for FAST tokenizers (other python-based tokenizer raise errors)\n",
        "# tokenizer = BertTokenizerFast.from_pretrained('wptoux/albert-chinese-large-qa')\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('hfl/chinese-roberta-wwm-ext-large')\n",
        "model = AutoModelForQuestionAnswering.from_pretrained('hfl/chinese-roberta-wwm-ext-large')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Td-GTmk5OW4"
      },
      "source": [
        "## Read Data\n",
        "\n",
        "- Training set: 26935 QA pairs\n",
        "- Dev set: 3523  QA pairs\n",
        "- Test set: 3492  QA pairs\n",
        "\n",
        "- {train/dev/test}_questions:\t\n",
        "  - List of dicts with the following keys:\n",
        "   - id (int)\n",
        "   - paragraph_id (int)\n",
        "   - question_text (string)\n",
        "   - answer_text (string)\n",
        "   - answer_start (int)\n",
        "   - answer_end (int)\n",
        "- {train/dev/test}_paragraphs: \n",
        "  - List of strings\n",
        "  - paragraph_ids in questions correspond to indexs in paragraphs\n",
        "  - A paragraph may be used by several questions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvX7hlepogvu"
      },
      "source": [
        "def read_data(file):\n",
        "    with open(file, 'r', encoding=\"utf-8\") as reader:\n",
        "        data = json.load(reader)\n",
        "    return data[\"questions\"], data[\"paragraphs\"]\n",
        "\n",
        "train_questions, train_paragraphs = read_data(\"hw7_train.json\")\n",
        "dev_questions, dev_paragraphs = read_data(\"hw7_dev.json\")\n",
        "test_questions, test_paragraphs = read_data(\"hw7_test.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fm0rpTHq0e4N"
      },
      "source": [
        "## Tokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTZ6B70Hoxie"
      },
      "source": [
        "# Tokenize questions and paragraphs separately\n",
        "# 「add_special_tokens」 is set to False since special tokens will be added when tokenized questions and paragraphs are combined in datset __getitem__ \n",
        "\n",
        "train_questions_tokenized = tokenizer([train_question[\"question_text\"] for train_question in train_questions], add_special_tokens=False)\n",
        "dev_questions_tokenized = tokenizer([dev_question[\"question_text\"] for dev_question in dev_questions], add_special_tokens=False)\n",
        "test_questions_tokenized = tokenizer([test_question[\"question_text\"] for test_question in test_questions], add_special_tokens=False) \n",
        "\n",
        "train_paragraphs_tokenized = tokenizer(train_paragraphs, add_special_tokens=False)\n",
        "dev_paragraphs_tokenized = tokenizer(dev_paragraphs, add_special_tokens=False)\n",
        "test_paragraphs_tokenized = tokenizer(test_paragraphs, add_special_tokens=False)\n",
        "\n",
        "# You can safely ignore the warning message as tokenized sequences will be futher processed in datset __getitem__ before passing to model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws8c8_4d5UCI"
      },
      "source": [
        "## Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjooag-Swnuh"
      },
      "source": [
        "class QA_Dataset(Dataset):\n",
        "    def __init__(self, split, questions, tokenized_questions, tokenized_paragraphs):\n",
        "        self.split = split\n",
        "        self.questions = questions\n",
        "        self.tokenized_questions = tokenized_questions\n",
        "        self.tokenized_paragraphs = tokenized_paragraphs\n",
        "        self.max_question_len = 40\n",
        "        self.max_paragraph_len = 300 #150\n",
        "        \n",
        "        ##### TODO: Change value of doc_stride #####\n",
        "        self.doc_stride = 100 # 70\n",
        "\n",
        "        # Input sequence length = [CLS] + question + [SEP] + paragraph + [SEP]\n",
        "        self.max_seq_len = 1 + self.max_question_len + 1 + self.max_paragraph_len + 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.questions[idx]\n",
        "        tokenized_question = self.tokenized_questions[idx] \n",
        "        tokenized_paragraph = self.tokenized_paragraphs[question[\"paragraph_id\"]]\n",
        "        \n",
        "        ##### TODO: Preprocessing #####\n",
        "        # Hint: How to prevent model from learning something it should not learn \n",
        "        # (也就是不要讓他以為answer is always in the middle)\n",
        "\n",
        "        if self.split == \"train\":\n",
        "            # Convert answer's start/end positions in paragraph_text to start/end positions in tokenized_paragraph  \n",
        "            answer_start_token = tokenized_paragraph.char_to_token(question[\"answer_start\"])\n",
        "            answer_end_token = tokenized_paragraph.char_to_token(question[\"answer_end\"])\n",
        "            \n",
        "            ###########\n",
        "            choice= random.choices([1/2, 1/3, 1/4, 1/5, 2, 3, 4, 5, 6], k = 1)[0]\n",
        "            ############\n",
        "            # A single window is obtained by slicing the portion of paragraph containing the answer\n",
        "            mid = (answer_start_token + answer_end_token) // 2\n",
        "            paragraph_start = max(0, min(mid - round(self.max_paragraph_len /choice) , len(tokenized_paragraph) - self.max_paragraph_len))\n",
        "            paragraph_end = paragraph_start + self.max_paragraph_len\n",
        "            \n",
        "            # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
        "            input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102] \n",
        "            input_ids_paragraph = tokenized_paragraph.ids[paragraph_start : paragraph_end] + [102]\t\t\n",
        "            \n",
        "            # Convert answer's start/end positions in tokenized_paragraph to start/end positions in the window  \n",
        "            answer_start_token += len(input_ids_question) - paragraph_start\n",
        "            answer_end_token += len(input_ids_question) - paragraph_start\n",
        "            \n",
        "            # Pad sequence and obtain inputs to model \n",
        "            input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
        "            return torch.tensor(input_ids), torch.tensor(token_type_ids), torch.tensor(attention_mask), answer_start_token, answer_end_token\n",
        "\n",
        "        # Validation/Testing\n",
        "        else:\n",
        "            paragraph_id = question[\"paragraph_id\"]\n",
        "            input_ids_list, token_type_ids_list, attention_mask_list = [], [], []\n",
        "            \n",
        "            # Paragraph is split into several windows, each with start positions separated by step \"doc_stride\"\n",
        "            for i in range(0, len(tokenized_paragraph), self.doc_stride):\n",
        "                \n",
        "                # Slice question/paragraph and add special tokens (101: CLS, 102: SEP)\n",
        "                input_ids_question = [101] + tokenized_question.ids[:self.max_question_len] + [102]\n",
        "                input_ids_paragraph = tokenized_paragraph.ids[i : i + self.max_paragraph_len] + [102]\n",
        "                \n",
        "                # Pad sequence and obtain inputs to model\n",
        "                input_ids, token_type_ids, attention_mask = self.padding(input_ids_question, input_ids_paragraph)\n",
        "                \n",
        "\n",
        "                input_ids_list.append(input_ids)\n",
        "                token_type_ids_list.append(token_type_ids)\n",
        "                attention_mask_list.append(attention_mask)\n",
        "            \n",
        "            return torch.tensor(input_ids_list), torch.tensor(token_type_ids_list), torch.tensor(attention_mask_list), paragraph_id\n",
        "\n",
        "    def padding(self, input_ids_question, input_ids_paragraph):\n",
        "        # Pad zeros if sequence length is shorter than max_seq_len\n",
        "        padding_len = self.max_seq_len - len(input_ids_question) - len(input_ids_paragraph)\n",
        "        # Indices of input sequence tokens in the vocabulary\n",
        "        input_ids = input_ids_question + input_ids_paragraph + [0] * padding_len\n",
        "        # Segment token indices to indicate first and second portions of the inputs. Indices are selected in [0, 1]\n",
        "        token_type_ids = [0] * len(input_ids_question) + [1] * len(input_ids_paragraph) + [0] * padding_len\n",
        "        # Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]\n",
        "        attention_mask = [1] * (len(input_ids_question) + len(input_ids_paragraph)) + [0] * padding_len\n",
        "        \n",
        "        return input_ids, token_type_ids, attention_mask\n",
        "\n",
        "train_batch_size = 8\n",
        "train_set = QA_Dataset(\"train\", train_questions, train_questions_tokenized, train_paragraphs_tokenized)\n",
        "dev_set = QA_Dataset(\"dev\", dev_questions, dev_questions_tokenized, dev_paragraphs_tokenized)\n",
        "test_set = QA_Dataset(\"test\", test_questions, test_questions_tokenized, test_paragraphs_tokenized)\n",
        "\n",
        "# Note: Do NOT change batch size of dev_loader / test_loader !\n",
        "# Although batch size=1, it is actually a batch consisting of several windows from the same QA pair\n",
        "train_loader = DataLoader(train_set, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
        "dev_loader = DataLoader(dev_set, batch_size=1, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_H1kqhR8CdM"
      },
      "source": [
        "## Function for Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eewy1MwVGXy"
      },
      "source": [
        "# answer = '白[UNK]紀中恐[UNK]大量滅絕'\n",
        "# gt = '歷史課本有教，白堊紀中恐龍大量滅絕，因此現在看不到'\n",
        "# # Hello I am (\\w*\\s?)him.\n",
        "# answer = answer.replace('[UNK]', '\\S')\n",
        "# print(answer)\n",
        "# match = re.search(answer, gt).group(0)\n",
        "# print(match)\n",
        "import regex as re "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqeA3PLPxOHu"
      },
      "source": [
        "def evaluate(data, output,isTest):\n",
        "    ##### TODO: Postprocessing #####\n",
        "    # There is a bug and room for improvement in postprocessing \n",
        "    # Hint: Open your prediction file to see what is wrong: \n",
        "    '''\n",
        "    start > end\n",
        "    [UNK] should be matched back to the original paragraph\n",
        "    '''\n",
        "    answer = ''\n",
        "    max_prob = float('-inf')\n",
        "    num_of_windows = data[0].shape[1]\n",
        "  \n",
        "    paragraph_id = data[-1].item()\n",
        "    for k in range(num_of_windows): # 5 in dev set \n",
        "        # Obtain answer by choosing the most probable start position / end position\n",
        "        start_prob, start_index = torch.max(output.start_logits[k], dim=0)\n",
        "        end_prob, end_index = torch.max(output.end_logits[k], dim=0)\n",
        "        # Probability of answer is calculated as sum of start_prob and end_prob\n",
        "        prob = start_prob + end_prob\n",
        "        \n",
        "        # Replace answer if calculated probability is larger than previous windows\n",
        "        if prob > max_prob:\n",
        "            max_prob = prob\n",
        "            # Convert tokens to chars (e.g. [1920, 7032] --> \"大 金\")\n",
        "            if start_index > end_index:\n",
        "              temp = start_index\n",
        "              start_index = end_index\n",
        "              end_index = temp \n",
        "            answer = tokenizer.decode(data[0][0][k][start_index : end_index + 1])\n",
        "    ########## Post-processing ################\n",
        "    answer = answer.replace(' ', '')\n",
        "    answer = answer.replace(',','')\n",
        "    answer = answer.replace('[SEP]','')\n",
        "    answer = answer.replace('[CLS]','')\n",
        "    answer= answer.replace('[PAD]','')\n",
        "    answer = answer.strip('的')\n",
        "    if '[UNK]' in answer: \n",
        "      print('Original:', answer, end = '|')\n",
        "      try:\n",
        "         # sometimes it can be 'MB' which are tokenized as one word but decoded to 2 words\n",
        "         # but set the boundary '[UNK]' to '\\S' to prevent over-matching\n",
        "        if not answer.endswith('[UNK]') and not answer.startswith('[UNK]'):\n",
        "          answer = answer.replace('[UNK]', '\\S+?') ####### to be fixed ######## \n",
        "        if (answer.startswith('[UNK]') or answer.endswith('[UNK]')):\n",
        "          answer = re.sub(r'^\\[UNK\\]', '@', answer) # \\: escape symbol # ^ matches beginning\n",
        "          answer = re.sub(r'\\[UNK\\]$', '@', answer) # $ matches end \n",
        "          answer = answer.replace('@', '\\S') # @ is a temp word marker \n",
        "          answer = answer.replace('[UNK]', '\\S+?') \n",
        "        # find the shortest match (else might return a paragraph)\n",
        "        if isTest:\n",
        "          matches = re.findall(answer, test_paragraphs[paragraph_id], overlapped = True) # match back to the original paragraph \n",
        "          answer = min(matches, key = len)\n",
        "        else:\n",
        "          matches = re.findall(answer, dev_paragraphs[paragraph_id], overlapped = True)\n",
        "          answer = min(matches, key = len)\n",
        "        print('Matched:', answer)\n",
        "      except: \n",
        "        print('No match.') # if a match is found, return it; else pass \n",
        "    return answer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OZIWSE2mTrk"
      },
      "source": [
        "!pip install lightning-bolts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IusnffTvmXp8"
      },
      "source": [
        "import pytorch_lightning as pl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzHQit6eMnKG"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Q-B6ka7xoCM"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau,ExponentialLR\n",
        "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
        "num_epoch = 3\n",
        "validation = True\n",
        "logging_step = 100\n",
        "learning_rate = 1e-4\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "accum_iter = 25 # virtually, real batch_size is batch_size * accum_iter \n",
        "# warmup_steps \n",
        "scheduler = LinearWarmupCosineAnnealingLR(optimizer,\n",
        "                                          warmup_start_lr = 1e-9,\n",
        "                                          warmup_epochs=200, # think of it as steps \n",
        "                                          max_epochs=num_epoch*len(train_loader)) # steps \n",
        "# scheduler = StepLR(optimizer, step_size= 1000, gamma=0.1) # 1000 steps 更新一次\n",
        "if fp16_training:\n",
        "    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader) \n",
        "\n",
        "model.train()\n",
        "\n",
        "print(\"Start Training ...\")\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    step = 1\n",
        "    train_loss = train_acc = 0\n",
        "    \n",
        "    for batch_idx, data in enumerate(tqdm(train_loader)):\t\n",
        "        # Load all data into GPU\n",
        "        data = [i.to(device) for i in data]\n",
        "        \n",
        "        # Model inputs: input_ids, token_type_ids, attention_mask, start_positions, end_positions (Note: only \"input_ids\" is mandatory)\n",
        "        # Model outputs: start_logits, end_logits, loss (return when start_positions/end_positions are provided)  \n",
        "        output = model(input_ids=data[0], \n",
        "                       token_type_ids=data[1], \n",
        "                       attention_mask=data[2], \n",
        "                       start_positions=data[3], \n",
        "                       end_positions=data[4])\n",
        "\n",
        "        # Choose the most probable start position / end position\n",
        "        start_index = torch.argmax(output.start_logits, dim=1) \n",
        "        end_index = torch.argmax(output.end_logits, dim=1)\n",
        "        \n",
        "        # Prediction is correct only if both start_index and end_index are correct\n",
        "        train_acc += ((start_index == data[3]) & (end_index == data[4])).float().mean()\n",
        "        train_loss += output.loss\n",
        "        # Gradient Accumulation for larger batch size (sample code: 16)\n",
        "        loss = output.loss\n",
        "        loss = loss / accum_iter \n",
        "        if fp16_training:\n",
        "            accelerator.backward(loss)\n",
        "        else:\n",
        "            loss.backward()\n",
        "        step += 1\n",
        "        # Grad accum: weights update condition \n",
        "        if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(train_loader)):\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        ##### TODO: Apply learning rate decay #####\n",
        "        scheduler.step()\n",
        "      \n",
        "        # Print training loss and accuracy over past logging step\n",
        "        if step % logging_step == 0:\n",
        "            print(f\"Epoch {epoch + 1} | Step {step} | loss = {train_loss.item() / logging_step:.3f}, acc = {train_acc / logging_step:.3f}\")\n",
        "            train_loss = train_acc = 0 \n",
        "\n",
        "    if validation:\n",
        "        print(\"Evaluating Dev Set ...\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            dev_acc = 0\n",
        "            for i, data in enumerate(tqdm(dev_loader)):\n",
        "              \n",
        "                output = model(input_ids=data[0].squeeze().to(device), token_type_ids=data[1].squeeze().to(device),\n",
        "                       attention_mask=data[2].squeeze().to(device))\n",
        "                # prediction is correct only if answer text exactly matches\n",
        "                dev_acc += evaluate(data, output, isTest = False) == dev_questions[i][\"answer_text\"]\n",
        "            print(f\"Validation | Epoch {epoch + 1} | acc = {dev_acc / len(dev_loader):.3f}\")\n",
        "        model.train()\n",
        "    \n",
        "# Save a model and its configuration file to the directory 「saved_model」 \n",
        "# i.e. there are two files under the direcory 「saved_model」: 「pytorch_model.bin」 and 「config.json」\n",
        "# Saved model can be re-loaded using 「model = BertForQuestionAnswering.from_pretrained(\"saved_model\")」\n",
        "print(\"Saving Model ...\")\n",
        "model_save_dir = \"saved_model\" \n",
        "model.save_pretrained(model_save_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMmdLOKBMsdE"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5scNKC9xz0C"
      },
      "source": [
        "print(\"Evaluating Test Set ...\")\n",
        "\n",
        "result = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data in tqdm(test_loader):\n",
        "        output = model(input_ids=data[0].squeeze(dim=0).to(device), token_type_ids=data[1].squeeze(dim=0).to(device),\n",
        "                       attention_mask=data[2].squeeze(dim=0).to(device))\n",
        "        result.append(evaluate(data, output, isTest = True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SnjWBSw3f-f"
      },
      "source": [
        "result_file = \"_.csv\"\n",
        "with open(result_file, 'w') as f:\t\n",
        "    f.write(\"ID,Answer\\n\")\n",
        "    for i, test_question in enumerate(test_questions):\n",
        "        # all preproc is done in evaluate()\n",
        "        f.write(f\"{test_question['id']},{result[i]}\\n\")\n",
        "print(f\"Completed! Result is in {result_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-XqUgA05GwH"
      },
      "source": [
        "from google.colab import files \n",
        "files.download(f'{result_file}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}