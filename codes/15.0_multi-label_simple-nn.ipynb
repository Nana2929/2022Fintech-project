{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15.0-multi-label-simple_nn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Product-type(category) task: Multi-label classification \n",
        "\n",
        "Author: Nana\n",
        "\n",
        "Date: 2022/4/14\n"
      ],
      "metadata": {
        "id": "RqnHVgUmy9nI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAz_HOx-fKBM",
        "outputId": "dcb04fb7-7f5c-492a-f43d-ad79107a6442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "maindir = '/content/drive/MyDrive/FinTech-final-project'\n",
        "datadir = f'{maindir}/data'\n",
        "spmdir = f'{maindir}/spm'\n",
        "modeldir = f'{maindir}/models'\n",
        "cat_df_path = f'{maindir}/東吳課程_發票資料集/品類資料集/cat_train_v2.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classifying fasttext embeddings\n",
        "MLknn\n",
        "https://notebook.community/scikit-multilearn/scikit-multilearn/docs/source/multilabelembeddings"
      ],
      "metadata": {
        "id": "JWPMgzB3gmaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install scikit-multilearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WlFrIBGg_uI",
        "outputId": "36709c53-b1af-4b79-b6b9-9908840a0d15"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 26.1 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20 kB 30.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30 kB 31.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 40 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 51 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 61 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 71 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 81 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 89 kB 5.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# openNE is required in skmultilearn\n",
        "!pip -q install networkx tensorflow\n",
        "!git clone https://github.com/thunlp/OpenNE/\n",
        "!pip -q install -e OpenNE/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mWxaY8AhzlT",
        "outputId": "6e225ee8-50b6-4e64-f481-8dacc578a8fc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 30 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |██▉                             | 40 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 51 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 61 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |█████                           | 71 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 81 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 92 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 102 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 112 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 122 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 133 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 143 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 153 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 163 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 174 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 184 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 194 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 204 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 215 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 225 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 235 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 245 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 256 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 266 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 276 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 286 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 296 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 307 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 317 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 327 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 337 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 348 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 358 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 368 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 378 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 389 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 399 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 409 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 419 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 430 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 440 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 450 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 460 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 462 kB 12.6 MB/s \n",
            "\u001b[?25hCloning into 'OpenNE'...\n",
            "remote: Enumerating objects: 2700, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 2700 (delta 5), reused 9 (delta 3), pack-reused 2677\u001b[K\n",
            "Receiving objects: 100% (2700/2700), 116.28 MiB | 15.62 MiB/s, done.\n",
            "Resolving deltas: 100% (1564/1564), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib, sys"
      ],
      "metadata": {
        "id": "_f3C-RkYfTM8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# required for skmultilearn (can skip if not using the package)\n",
        "# # follow the instructions on https://github.com/thunlp/OpenNE\n",
        "!pip install -r requirements.txt\n",
        "%cd OpenNE/src\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "oLc0woPjkKqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Data (X, y) \n",
        "\n",
        "http://scikit.ml/multilabeldnn.html#Multi-label-deep-learning-with-scikit-multilearn\n",
        "\n"
      ],
      "metadata": {
        "id": "FttSF77Iw8oZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fmodelpath = f'{modeldir}/fasttext.model'\n",
        "ypath = f'{datadir}/category/category_labels.pkl'"
      ],
      "metadata": {
        "id": "JkKDJX3cFDqD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib \n",
        "from collections import defaultdict\n",
        "LabelsList = joblib.load(ypath)"
      ],
      "metadata": {
        "id": "kisYJvFXFG3o"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CatsDict = defaultdict(list)\n",
        "for x in LabelsList:\n",
        "  # print(x.keys())\n",
        "  for cat in x['product']:\n",
        "    # print(x['name'])\n",
        "    CatsDict[cat].append(x['name'])"
      ],
      "metadata": {
        "id": "kp3cNZyoFI78"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "print('# of categories:', len(CatsDict.keys()))\n",
        "for k, v in CatsDict.items():\n",
        "  print(f\"Category: {k}, Length: {len(v)}, \\nExamples:{random.sample(v, min(10, len(v)))}\")\n",
        "  print('---------------')\n",
        "  # break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5a8B4wVF0t6",
        "outputId": "ddbe9e86-3833-476d-e5d9-d1e5fd4d47b1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# of categories: 217\n",
            "Category: 人工淚液, Length: 121, \n",
            "Examples:['R佐藤視朗點眼液15ml', 'R參天柔潤人工淚液點眼液5ml-4入', '61213', '(R)新鮮視OA點眼液', 'Sato 視朗眼液15', '愛力根優麗舒(R)單支裝眼用點眼液', 'C-SINOMIN止膿敏點眼液15ML(裸)', 'R參天柔潤人工淚液點眼液 5ml 4入', 'R視舒坦人工淚液點眼液10ml', '參天柔潤人工淚液點眼液5ml-4入']\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(f\"{datadir}/category/category_dictionary.json\", \"w\") as outfile:\n",
        "    json.dump(CatsDict, outfile, ensure_ascii = False, indent = 4)"
      ],
      "metadata": {
        "id": "u3zqNcOSHrKw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# average the token vectors to form X's feature vector\n",
        "# use the binary tuple as label\n",
        "from gensim.models import FastText\n",
        "# should save keyedvectors only if there's no further training \n",
        "model = FastText.load(fmodelpath) "
      ],
      "metadata": {
        "id": "Z_SgjPyAw_gr"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## segmented, should have saved the segmented results for this\n",
        "# !pip install -q -U ckip-transformers\n",
        "# from ckip_transformers.nlp import CkipWordSegmenter\n",
        "# ws_driver = CkipWordSegmenter(level=3, device = 0)\n",
        "# name_texts = [x['name'] for x in LabelsList]\n",
        "# ws = ws_driver(name_texts) !nvidia-smi # k80要20分鐘"
      ],
      "metadata": {
        "id": "Gp6w_Mlb0gOf"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# parameters\n",
        "emb_dim = 200 # (see notebook 12.0)\n",
        "Xlen = len(LabelsList)\n",
        "nclass = len(LabelsList[0]['labels'])\n",
        "\n",
        "# make X(Emb matrix), y (label matrix)\n",
        "Emb = np.zeros((Xlen, emb_dim))\n",
        "Labels = np.zeros((Xlen, nclass))\n",
        "\n",
        "def get_avg_embeddings(tokenlist):\n",
        "  # if no token has embedding available, yields np.zeros(emb_dim)\n",
        "  return np.mean([model.wv[tok] if tok in model.wv else np.zeros(emb_dim) for tok in tokenlist], axis = 0)\n",
        "\n",
        "def get_weighted_embeddings(tokenlist):\n",
        "  # ref: https://stackoverflow.com/questions/29330792/weighted-averaging-a-list\n",
        "  rate = np.ones(len(tokenlist))\n",
        "  mid = int(len(rate)/2)\n",
        "  # category name通常出現在產品名稱後半部\n",
        "  rate = np.append(rate[:mid], (rate[mid:]+1))\n",
        "  # print(rate)\n",
        "  X = [model.wv[tok] if tok in model.wv else np.zeros(emb_dim) for tok in tokenlist]\n",
        "  return np.average(X, \n",
        "                    axis = 0,\n",
        "                    weights=rate)"
      ],
      "metadata": {
        "id": "hGdSlaU1zZxy"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(Xlen):\n",
        "    Emb[i,:] = get_weighted_embeddings(LabelsList[i]['seg_name'])\n",
        "    Labels[i,:] = LabelsList[i]['labels']"
      ],
      "metadata": {
        "id": "RJm5jX1BLFzG"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_weighted_embeddings(['黑人','牙膏'])"
      ],
      "metadata": {
        "id": "4o64wr8FK9Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.wv['黑人'])\n",
        "print(model.wv['牙膏'])"
      ],
      "metadata": {
        "id": "kwUMWE2TLQyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "stack a simple few-layer neural network "
      ],
      "metadata": {
        "id": "4CtUBbO2G1OQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWEow1eo3WKY",
        "outputId": "bc0a2297-0630-4832-cad4-c560936fcbaa"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 750.6 MB 12 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.11.0 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# seeding; normally not working in colab\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "myseed = 42\n",
        "torch.manual_seed(myseed)\n",
        "torch.cuda.manual_seed(myseed)\n",
        "torch.cuda.manual_seed_all(myseed)\n",
        "np.random.seed(myseed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "yWKB3A0X4ctR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# 一些分工：做k-fold、\n",
        "X_train, X_test, y_train, y_test = train_test_split(Emb, Labels, test_size=0.2, \n",
        "                                                    random_state=myseed)\n",
        "input_dim = X_train.shape[1]\n",
        "hd1, hd2 = 128, 256\n",
        "dropout = 0.5\n",
        "output_dim = nclass # originally len(np.unique(y_train.rows)) as http://scikit.ml/multilabeldnn.html#Multi-label-deep-learning-with-scikit-multilearn \n",
        "# states, but the logic is (if I understand it correctly) very weird"
      ],
      "metadata": {
        "id": "IfIk9-A4jE9A"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CatDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.data = torch.from_numpy(X).float()\n",
        "        if y is not None:\n",
        "            y = y.astype(np.int)\n",
        "            self.label = torch.from_numpy(y).float()\n",
        "        else:\n",
        "            self.label = None\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is not None:\n",
        "            return self.data[idx], self.label[idx]\n",
        "        else:\n",
        "            return self.data[idx]\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n"
      ],
      "metadata": {
        "id": "VJrMKgNNCckM"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "batch_size = 200\n",
        "train_set = CatDataset(X_train, y_train)\n",
        "val_set = CatDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=False)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "NHhkPETyC2yF"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture(s): Simple Network (linear layers)\n"
      ],
      "metadata": {
        "id": "hKme9QQeOg7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLabelClassifier(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim=input_dim,\n",
        "            hd1 = hd1,\n",
        "            hd2 = hd2,\n",
        "            output_dim=output_dim,\n",
        "            dropout=dropout,\n",
        "    ):\n",
        "        super(MultiLabelClassifier, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # 只有兩層fc layers (shallow)，主要是看fasttext embedding的效果\n",
        "        self.hidden1 = nn.Linear(input_dim, hd1)\n",
        "        self.hidden2 = nn.Linear(hd1, hd2)\n",
        "        self.output = nn.Linear(hd2, output_dim)\n",
        "\n",
        "    def forward(self, X, **kwargs):\n",
        "        X = self.hidden1(X)\n",
        "        X = self.hidden2(X)\n",
        "        X = F.relu(X)\n",
        "        X = self.dropout(X)\n",
        "        # 過softmax拿機率值\n",
        "        X = F.softmax(self.output(X), dim=-1)\n",
        "        return X"
      ],
      "metadata": {
        "id": "wMuzAv4v4_YB"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References: \n",
        "https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
        "https://towardsdatascience.com/\n",
        "\n",
        "lstm-text-classification-using-pytorch-2c6c657f8fc0\n",
        "\n",
        "My machine learning assignments"
      ],
      "metadata": {
        "id": "kvLa_WTeR4e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from skmultilearn.problem_transform import LabelPowerset\n",
        "# training parameters\n",
        "THRESHOLD = 0.3\n",
        "epochs = 100\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "learning_rate = 5e-4       # learning rate\n",
        "net = MultiLabelClassifier().to(device)\n",
        "criterion = nn.BCELoss() \n",
        "weight_decay = 0.1\n",
        "optimizer = torch.optim.AdamW(net.parameters(), \n",
        "                              lr=learning_rate, \n",
        "                              weight_decay=weight_decay)"
      ],
      "metadata": {
        "id": "p-dS9kqY6njW"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple version: Macro f1: 0.2 ?? "
      ],
      "metadata": {
        "id": "4d49aU0KOmPC"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "2iAZwCtUKQVn"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "from sklearn.metrics import f1_score\n",
        "best_acc = 0.0\n",
        "for epoch in range(epochs):\n",
        "    train_acc, val_acc = 0.0, 0.0\n",
        "    train_loss, val_loss = 0.0,0.0\n",
        "    # training\n",
        "    net.train() # set the model to training mode\n",
        "    for i, data in enumerate(train_loader):\n",
        "        inputs, labels = data\n",
        "        # print('l:', labels.shape)\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() \n",
        "        outputs = net(inputs) \n",
        "        # print('o:', outputs.shape, 'l:', labels.shape)\n",
        "        batch_loss = criterion(outputs, labels)\n",
        "        \n",
        "        # THRESHOLD: all labels with probabilities higher than it are considered predicted labels \n",
        "        # and others are skipped. We are using a threshold value of 0.5.\n",
        "        pred = np.array(outputs.cpu().detach().numpy() > THRESHOLD, dtype=float)\n",
        "        labels = labels.cpu().detach().numpy()\n",
        "        train_acc += f1_score(labels, pred, average='macro')\n",
        "        # print(f1_score(labels, pred, average='macro'))\n",
        "        # compute macro_f1\n",
        "        # print(labels.shape, pred.shape)\n",
        "        # print(f1_score(labels, pred, average='macro'))\n",
        "        batch_loss.backward() \n",
        "        optimizer.step() \n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    # validation\n",
        "    if len(val_set) > 0:\n",
        "        net.eval() # set the model to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader):\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs =net(inputs)\n",
        "                batch_loss = criterion(outputs, labels) \n",
        "                val_pred = np.array(outputs.cpu().detach().numpy() > THRESHOLD, dtype=float)\n",
        "                labels = labels.cpu().detach().numpy()\n",
        "                val_acc += f1_score(labels, val_pred, average='macro')\n",
        "                # compute macro_f1\n",
        "                val_loss += batch_loss.item()\n",
        "\n",
        "            print('[{:03d}/{:03d}] Train Macro f1: {:3.6f} Loss: {:3.6f} | Val Macro f1: {:3.6f} loss: {:3.6f}'.format(\n",
        "                epoch + 1, epochs, train_acc/len(train_loader), train_loss/len(train_loader), val_acc/len(val_loader), val_loss/len(val_loader)\n",
        "            ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DptuDrTNBFri",
        "outputId": "d39658fc-04e7-4021-9ba3-c959658ab726"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[001/100] Train Macro f1: 0.072467 Loss: 0.016402 | Val Macro f1: 0.147684 loss: 0.010557\n",
            "[002/100] Train Macro f1: 0.161240 Loss: 0.010588 | Val Macro f1: 0.197878 loss: 0.008691\n",
            "[003/100] Train Macro f1: 0.195289 Loss: 0.009274 | Val Macro f1: 0.223527 loss: 0.007875\n",
            "[004/100] Train Macro f1: 0.214887 Loss: 0.008524 | Val Macro f1: 0.238000 loss: 0.007356\n",
            "[005/100] Train Macro f1: 0.226800 Loss: 0.008064 | Val Macro f1: 0.248521 loss: 0.007014\n",
            "[006/100] Train Macro f1: 0.237154 Loss: 0.007720 | Val Macro f1: 0.255675 loss: 0.006745\n",
            "[007/100] Train Macro f1: 0.242494 Loss: 0.007467 | Val Macro f1: 0.261538 loss: 0.006518\n",
            "[008/100] Train Macro f1: 0.250259 Loss: 0.007226 | Val Macro f1: 0.265354 loss: 0.006332\n",
            "[009/100] Train Macro f1: 0.254694 Loss: 0.007026 | Val Macro f1: 0.269082 loss: 0.006205\n",
            "[010/100] Train Macro f1: 0.259037 Loss: 0.006871 | Val Macro f1: 0.274297 loss: 0.006063\n",
            "[011/100] Train Macro f1: 0.262538 Loss: 0.006741 | Val Macro f1: 0.275832 loss: 0.005957\n",
            "[012/100] Train Macro f1: 0.264685 Loss: 0.006645 | Val Macro f1: 0.279023 loss: 0.005860\n",
            "[013/100] Train Macro f1: 0.267414 Loss: 0.006536 | Val Macro f1: 0.281411 loss: 0.005772\n",
            "[014/100] Train Macro f1: 0.270572 Loss: 0.006451 | Val Macro f1: 0.283260 loss: 0.005702\n",
            "[015/100] Train Macro f1: 0.272169 Loss: 0.006343 | Val Macro f1: 0.286066 loss: 0.005608\n",
            "[016/100] Train Macro f1: 0.275247 Loss: 0.006245 | Val Macro f1: 0.286314 loss: 0.005570\n",
            "[017/100] Train Macro f1: 0.276563 Loss: 0.006180 | Val Macro f1: 0.287774 loss: 0.005497\n",
            "[018/100] Train Macro f1: 0.278484 Loss: 0.006108 | Val Macro f1: 0.288886 loss: 0.005450\n",
            "[019/100] Train Macro f1: 0.279561 Loss: 0.006053 | Val Macro f1: 0.291463 loss: 0.005376\n",
            "[020/100] Train Macro f1: 0.280692 Loss: 0.005999 | Val Macro f1: 0.291831 loss: 0.005340\n",
            "[021/100] Train Macro f1: 0.282073 Loss: 0.005966 | Val Macro f1: 0.293392 loss: 0.005294\n",
            "[022/100] Train Macro f1: 0.283854 Loss: 0.005891 | Val Macro f1: 0.294197 loss: 0.005259\n",
            "[023/100] Train Macro f1: 0.284233 Loss: 0.005857 | Val Macro f1: 0.296232 loss: 0.005223\n",
            "[024/100] Train Macro f1: 0.286031 Loss: 0.005820 | Val Macro f1: 0.296057 loss: 0.005198\n",
            "[025/100] Train Macro f1: 0.287512 Loss: 0.005767 | Val Macro f1: 0.298013 loss: 0.005161\n",
            "[026/100] Train Macro f1: 0.287990 Loss: 0.005739 | Val Macro f1: 0.298517 loss: 0.005119\n",
            "[027/100] Train Macro f1: 0.289643 Loss: 0.005674 | Val Macro f1: 0.297715 loss: 0.005114\n",
            "[028/100] Train Macro f1: 0.289333 Loss: 0.005664 | Val Macro f1: 0.299777 loss: 0.005069\n",
            "[029/100] Train Macro f1: 0.290763 Loss: 0.005609 | Val Macro f1: 0.299686 loss: 0.005037\n",
            "[030/100] Train Macro f1: 0.291426 Loss: 0.005604 | Val Macro f1: 0.301374 loss: 0.005017\n",
            "[031/100] Train Macro f1: 0.292624 Loss: 0.005553 | Val Macro f1: 0.301686 loss: 0.004976\n",
            "[032/100] Train Macro f1: 0.292751 Loss: 0.005524 | Val Macro f1: 0.302213 loss: 0.004974\n",
            "[033/100] Train Macro f1: 0.294971 Loss: 0.005494 | Val Macro f1: 0.303952 loss: 0.004937\n",
            "[034/100] Train Macro f1: 0.295055 Loss: 0.005454 | Val Macro f1: 0.303961 loss: 0.004922\n",
            "[035/100] Train Macro f1: 0.295255 Loss: 0.005456 | Val Macro f1: 0.304504 loss: 0.004888\n",
            "[036/100] Train Macro f1: 0.295979 Loss: 0.005403 | Val Macro f1: 0.303971 loss: 0.004889\n",
            "[037/100] Train Macro f1: 0.296651 Loss: 0.005412 | Val Macro f1: 0.306466 loss: 0.004846\n",
            "[038/100] Train Macro f1: 0.298223 Loss: 0.005350 | Val Macro f1: 0.306225 loss: 0.004821\n",
            "[039/100] Train Macro f1: 0.297294 Loss: 0.005360 | Val Macro f1: 0.307424 loss: 0.004819\n",
            "[040/100] Train Macro f1: 0.298002 Loss: 0.005317 | Val Macro f1: 0.309279 loss: 0.004793\n",
            "[041/100] Train Macro f1: 0.299430 Loss: 0.005303 | Val Macro f1: 0.308544 loss: 0.004780\n",
            "[042/100] Train Macro f1: 0.300033 Loss: 0.005267 | Val Macro f1: 0.307754 loss: 0.004766\n",
            "[043/100] Train Macro f1: 0.299994 Loss: 0.005273 | Val Macro f1: 0.308566 loss: 0.004754\n",
            "[044/100] Train Macro f1: 0.300164 Loss: 0.005244 | Val Macro f1: 0.309165 loss: 0.004712\n",
            "[045/100] Train Macro f1: 0.300554 Loss: 0.005226 | Val Macro f1: 0.308985 loss: 0.004719\n",
            "[046/100] Train Macro f1: 0.300481 Loss: 0.005211 | Val Macro f1: 0.310428 loss: 0.004694\n",
            "[047/100] Train Macro f1: 0.300891 Loss: 0.005189 | Val Macro f1: 0.310286 loss: 0.004685\n",
            "[048/100] Train Macro f1: 0.301918 Loss: 0.005183 | Val Macro f1: 0.310924 loss: 0.004678\n",
            "[049/100] Train Macro f1: 0.302761 Loss: 0.005156 | Val Macro f1: 0.311083 loss: 0.004689\n",
            "[050/100] Train Macro f1: 0.303295 Loss: 0.005152 | Val Macro f1: 0.312334 loss: 0.004649\n",
            "[051/100] Train Macro f1: 0.303429 Loss: 0.005153 | Val Macro f1: 0.311492 loss: 0.004652\n",
            "[052/100] Train Macro f1: 0.303314 Loss: 0.005105 | Val Macro f1: 0.311997 loss: 0.004634\n",
            "[053/100] Train Macro f1: 0.304224 Loss: 0.005107 | Val Macro f1: 0.312418 loss: 0.004617\n",
            "[054/100] Train Macro f1: 0.303977 Loss: 0.005074 | Val Macro f1: 0.311212 loss: 0.004615\n",
            "[055/100] Train Macro f1: 0.304223 Loss: 0.005067 | Val Macro f1: 0.310317 loss: 0.004636\n",
            "[056/100] Train Macro f1: 0.304765 Loss: 0.005060 | Val Macro f1: 0.312178 loss: 0.004618\n",
            "[057/100] Train Macro f1: 0.305218 Loss: 0.005071 | Val Macro f1: 0.312160 loss: 0.004604\n",
            "[058/100] Train Macro f1: 0.304852 Loss: 0.005032 | Val Macro f1: 0.313598 loss: 0.004573\n",
            "[059/100] Train Macro f1: 0.305271 Loss: 0.005037 | Val Macro f1: 0.314333 loss: 0.004551\n",
            "[060/100] Train Macro f1: 0.306577 Loss: 0.004995 | Val Macro f1: 0.312455 loss: 0.004580\n",
            "[061/100] Train Macro f1: 0.306079 Loss: 0.005003 | Val Macro f1: 0.312923 loss: 0.004556\n",
            "[062/100] Train Macro f1: 0.306301 Loss: 0.005005 | Val Macro f1: 0.314262 loss: 0.004544\n",
            "[063/100] Train Macro f1: 0.306683 Loss: 0.004988 | Val Macro f1: 0.314318 loss: 0.004521\n",
            "[064/100] Train Macro f1: 0.308024 Loss: 0.004954 | Val Macro f1: 0.315786 loss: 0.004504\n",
            "[065/100] Train Macro f1: 0.307531 Loss: 0.004978 | Val Macro f1: 0.315142 loss: 0.004507\n",
            "[066/100] Train Macro f1: 0.308795 Loss: 0.004934 | Val Macro f1: 0.315642 loss: 0.004497\n",
            "[067/100] Train Macro f1: 0.306973 Loss: 0.004948 | Val Macro f1: 0.314536 loss: 0.004504\n",
            "[068/100] Train Macro f1: 0.308738 Loss: 0.004926 | Val Macro f1: 0.315402 loss: 0.004504\n",
            "[069/100] Train Macro f1: 0.308817 Loss: 0.004925 | Val Macro f1: 0.315850 loss: 0.004478\n",
            "[070/100] Train Macro f1: 0.308524 Loss: 0.004916 | Val Macro f1: 0.315782 loss: 0.004461\n",
            "[071/100] Train Macro f1: 0.308119 Loss: 0.004917 | Val Macro f1: 0.315787 loss: 0.004456\n",
            "[072/100] Train Macro f1: 0.309328 Loss: 0.004904 | Val Macro f1: 0.316399 loss: 0.004448\n",
            "[073/100] Train Macro f1: 0.309098 Loss: 0.004893 | Val Macro f1: 0.316090 loss: 0.004427\n",
            "[074/100] Train Macro f1: 0.309849 Loss: 0.004874 | Val Macro f1: 0.316166 loss: 0.004444\n",
            "[075/100] Train Macro f1: 0.310186 Loss: 0.004839 | Val Macro f1: 0.316977 loss: 0.004424\n",
            "[076/100] Train Macro f1: 0.310128 Loss: 0.004842 | Val Macro f1: 0.316526 loss: 0.004424\n",
            "[077/100] Train Macro f1: 0.309670 Loss: 0.004860 | Val Macro f1: 0.316909 loss: 0.004425\n",
            "[078/100] Train Macro f1: 0.309856 Loss: 0.004825 | Val Macro f1: 0.317179 loss: 0.004418\n",
            "[079/100] Train Macro f1: 0.310528 Loss: 0.004828 | Val Macro f1: 0.316981 loss: 0.004410\n",
            "[080/100] Train Macro f1: 0.311380 Loss: 0.004831 | Val Macro f1: 0.317038 loss: 0.004400\n",
            "[081/100] Train Macro f1: 0.311075 Loss: 0.004798 | Val Macro f1: 0.318625 loss: 0.004368\n",
            "[082/100] Train Macro f1: 0.311749 Loss: 0.004806 | Val Macro f1: 0.318308 loss: 0.004399\n",
            "[083/100] Train Macro f1: 0.311210 Loss: 0.004836 | Val Macro f1: 0.319073 loss: 0.004375\n",
            "[084/100] Train Macro f1: 0.311277 Loss: 0.004802 | Val Macro f1: 0.317973 loss: 0.004394\n",
            "[085/100] Train Macro f1: 0.310286 Loss: 0.004809 | Val Macro f1: 0.318702 loss: 0.004360\n",
            "[086/100] Train Macro f1: 0.310881 Loss: 0.004787 | Val Macro f1: 0.318544 loss: 0.004360\n",
            "[087/100] Train Macro f1: 0.311677 Loss: 0.004796 | Val Macro f1: 0.317920 loss: 0.004364\n",
            "[088/100] Train Macro f1: 0.312001 Loss: 0.004768 | Val Macro f1: 0.318551 loss: 0.004349\n",
            "[089/100] Train Macro f1: 0.311672 Loss: 0.004782 | Val Macro f1: 0.319752 loss: 0.004360\n",
            "[090/100] Train Macro f1: 0.311699 Loss: 0.004791 | Val Macro f1: 0.319719 loss: 0.004340\n",
            "[091/100] Train Macro f1: 0.312729 Loss: 0.004773 | Val Macro f1: 0.318903 loss: 0.004353\n",
            "[092/100] Train Macro f1: 0.311935 Loss: 0.004747 | Val Macro f1: 0.320286 loss: 0.004320\n",
            "[093/100] Train Macro f1: 0.311717 Loss: 0.004761 | Val Macro f1: 0.320200 loss: 0.004303\n",
            "[094/100] Train Macro f1: 0.313245 Loss: 0.004736 | Val Macro f1: 0.319622 loss: 0.004318\n",
            "[095/100] Train Macro f1: 0.313505 Loss: 0.004726 | Val Macro f1: 0.321295 loss: 0.004292\n",
            "[096/100] Train Macro f1: 0.313052 Loss: 0.004735 | Val Macro f1: 0.320086 loss: 0.004328\n",
            "[097/100] Train Macro f1: 0.312614 Loss: 0.004736 | Val Macro f1: 0.320141 loss: 0.004321\n",
            "[098/100] Train Macro f1: 0.313272 Loss: 0.004726 | Val Macro f1: 0.320181 loss: 0.004313\n",
            "[099/100] Train Macro f1: 0.313272 Loss: 0.004713 | Val Macro f1: 0.320171 loss: 0.004330\n",
            "[100/100] Train Macro f1: 0.313756 Loss: 0.004695 | Val Macro f1: 0.320545 loss: 0.004302\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(labels, val_pred, average='macro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2GvYr7uSz7e",
        "outputId": "a6de9688-110c-4019-e249-8b9aa36e3c05"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1675840978593272"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred[1,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToZ7arz1S23e",
        "outputId": "313a3176-3f91-4bae-cdb5-d122af131e87"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "# multilabel classification\n",
        "y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
        "y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
        "f1_score(y_true, y_pred, average='macro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx4r7mIC9iBv",
        "outputId": "27f70072-06a7-47dc-e751-0b2513beab14"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7777777777777777"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking example reference: \n",
        "http://scikit.ml/multilabeldnn.html#Multi-label-deep-learning-with-scikit-multilearn\n",
        "(which doesn't seem to have the logic our task requires)"
      ],
      "metadata": {
        "id": "9_lOj32TJvZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arff\n",
        "from skmultilearn.dataset import load_dataset\n",
        "X_train, y_train, feature_names, label_names = load_dataset('emotions', 'train')\n",
        "X_test, y_test, _, _ = load_dataset('emotions', 'test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aez1JtPm-4X7",
        "outputId": "1e848c7f-c0eb-4fb3-c8ea-5b3a82290430"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arff\n",
            "  Downloading arff-0.9.tar.gz (4.7 kB)\n",
            "Building wheels for collected packages: arff\n",
            "  Building wheel for arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for arff: filename=arff-0.9-py3-none-any.whl size=4971 sha256=e32881b68341e9e34382d8239db4b8531a7b17ffb35a7d841e26a72a76071a58\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/81/bd/4ae90e24ba860304e375da219f9205b2586dbee255f3ee70e2\n",
            "Successfully built arff\n",
            "Installing collected packages: arff\n",
            "Successfully installed arff-0.9\n",
            "emotions:train - does not exists downloading\n",
            "Downloaded emotions-train\n",
            "emotions:test - does not exists downloading\n",
            "Downloaded emotions-test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.rows # ?? probably not what I expected"
      ],
      "metadata": {
        "id": "dh-adIk3-4sy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}